# Jobs y CronJobs en Kubernetes

Al igual que un Deployment, un **Job** es un recurso de Kubernetes.

Un Job crea uno o m√°s Pods para ejecutar una tarea espec√≠fica y, una vez que la tarea finaliza, elimina autom√°ticamente los Pods.

A diferencia de otros controladores de Kubernetes, un Job **no mantiene un estado deseado**, sino que gestiona una tarea **hasta que se completa**.

---

## ¬øC√≥mo funciona un Job?

En su forma m√°s simple, un Job:

- Crea un solo Pod.
- Ejecuta una tarea dentro de ese Pod.
- Supervisa la tarea hasta que finaliza.
- Elimina el Pod al terminar.

Si un Pod falla:
- El Job controller detecta que la tarea no termin√≥.
- Reprograma un nuevo Pod en otro nodo.
- Contin√∫a hasta que la tarea se complete con √©xito.

Ejemplo:  
Un usuario sube un video para ser convertido (transcoding).  
Ese proceso es una tarea que puede ejecutarse mediante un Job.
Image02.png

---

## Tipos de Jobs

### 1. Jobs no paralelos

Son el tipo m√°s simple.

Caracter√≠sticas:
- Ejecutan una tarea una sola vez.
- Solo usan un Pod.
- El Job finaliza cuando el Pod termina exitosamente o se alcanza el n√∫mero requerido de completions.

Ejemplos:
- Procesamiento de im√°genes
- Migraci√≥n de datos

Si el Pod falla, se recrea autom√°ticamente.

---

### 2. Jobs paralelos

Ejecutan m√∫ltiples Pods en paralelo.

Caracter√≠sticas:
- Varios Pods trabajan de forma independiente.
- Existe un numero predefinido para limitir el numero de finalizaciones.
- El Job finaliza cuando se alcanza el n√∫mero total de tareas completadas exitosas.

Son √∫tiles cuando las tareas se tienen que realizar m√°s de una vez:
- Redimensionamiento masivo de im√°genes
- C√°lculos cient√≠ficos en paralelo

Existen otros subtipos, como:
- Work queues
- Indexed jobs  
(no incluidos en este nivel).

---

## Componentes del manifiesto de un Job

Image03.png

El **tipo de objeto** se define en el campo kind: Job, este define que tipo de proceso en lotes el job representa y como deber√≠a ser tratado.

El comportamiento del Job se define en spec

Dentro del `spec` encontramos:

**Pod template**: el molde que define c√≥mo ser√°n los Pods creados por el Job.

**RestartPolicy**: define qu√© ocurre si falla un contenedor. Valores posibles:
- `Never`:  
  Si un contenedor falla, el Pod completo falla.
  El Job controller crea un Pod nuevo.

- `OnFailure`:  
  El Pod se mantiene en el nodo y el contenedor se reinicia.

**backOffLimit**: define cu√°ntos intentos fallidos se permiten antes de que Kubernetes marque el Job como fallido.

## Jobs paralelos: completions y parallelism

Image04.png

Para ejecutar un Job en paralelo se usan dos valores:

- `parallelism`:  
  N√∫mero de Pods que se ejecutan al mismo tiempo.

- `completions`:  
  N√∫mero total de tareas exitosas requeridas.

Funcionamiento:
1. Kubernetes crea tantos Pods como indique `parallelism`.
2. Cuando un Pod termina, se crea otro nuevo.
3. Esto contin√∫a hasta alcanzar el n√∫mero indicado en `completions`.

---

## Inspecci√≥n y eliminaci√≥n de Jobs

Puedes ver el estado de un Job con:

kubectl describe job <nombre>

Puedes ver los Pods del Job usando selectores de etiquetas:

kubectl get pods -l job-name=<nombre>

Tambi√©n puedes acceder a esta informaci√≥n desde la consola de Google Cloud.

---

## Eliminar un Job

Para eliminar un Job:

kubectl delete job <nombre>

Esto elimina:
- El Job
- Sus Pods asociados

Si quieres conservar los Pods:

kubectl delete job <nombre> --cascade=false

Tambi√©n puedes hacerlo desde la consola de Google Cloud.

---

## CronJobs

Los **CronJobs** permiten ejecutar Jobs autom√°ticamente en horarios espec√≠ficos.

Se basan en el formato **cron**, un lenguaje de programaci√≥n de tiempos.

---

## Sintaxis Cron

La sintaxis cron se compone de varios campos separados por espacios y cada uno controla un aspecto del tiempo de ejecuci√≥n.

Ejemplo general:

Cada parte representa:
- Minuto
- Hora
- D√≠a del mes
- Mes
- D√≠a de la semana

---

## Caracter√≠sticas del formato Cron

### Asterisco (*)
Representa todos los valores posibles.

Ejemplos:
- Cada minuto
- Todas las horas

---

### Listas y rangos

Para valores espec√≠ficos: 1,3,7
Para rangos: 1-5
---
### Intervalos con /
Para repetir en intervalos:*/5 Significa: cada 5 unidades (minutos, horas, etc.).

Tambi√©n funciona con rangos:1-10/2

Ejecuta cada 2 unidades dentro del rango.

---
# üöÄ Escalado de Cl√∫steres en GKE (Google Kubernetes Engine)

## 1. Modos de Operaci√≥n
Existen dos formas principales de gestionar el escalado en GKE:

* **Autopilot Mode:** Escala el cl√∫ster autom√°ticamente seg√∫n la demanda (manos libres).
* **Standard Mode:** Te da el control total. T√∫ decides si escalas manualmente o configuras el escalador autom√°tico.

## 2. Conceptos Clave: Node Pools (Grupos de Nodos)
Un cl√∫ster se compone de uno o m√°s **Node Pools**.

* **Definici√≥n:** Un grupo de nodos con la misma configuraci√≥n (tipo de m√°quina, disco, etc.) dentro de un cl√∫ster.
* **Creaci√≥n:** Al crear un cl√∫ster, se genera un node pool por defecto. Puedes agregar pools personalizados con diferentes tama√±os y tipos despu√©s.
* **Regla de Escalado a Cero:**
    * Los *node pools* individuales pueden escalarse a **0**.
    * El *cl√∫ster* en s√≠ **no** puede cerrarse totalmente; necesita al menos 1 nodo activo para correr los Pods del sistema.

## 3. Tipos de Escalado en Standard Mode

### A. Escalado Manual
Se realiza a trav√©s de la consola de Google Cloud o Cloud Shell.
* **Comando:** `gcloud container clusters resize`.
* **Comportamiento:**
    * Elimina instancias de forma aleatoria.
    * Los Pods en ejecuci√≥n terminan de forma ordenada (*gracefully*).

### B. Cluster Autoscaler (Escalado Autom√°tico)
Caracter√≠stica opcional (desactivada por defecto) que ajusta el tama√±o del pool seg√∫n la demanda de recursos.

#### ¬øC√≥mo funciona el "Scale Up" (Aumento)?
1. Si los Pods no tienen recursos suficientes, el programador (*scheduler*) no puede asignarlos.
2. El estado del Pod se marca como **Unschedulable** (No programable).
3. El Autoscaler detecta esto y **a√±ade nuevos nodos** autom√°ticamente.

#### ¬øC√≥mo funciona el "Scale Down" (Reducci√≥n)?
El Autoscaler busca nodos "prescindibles" cada **10 segundos**. Un nodo es candidato a eliminaci√≥n si cumple **todas** estas condiciones:
1. Uso de CPU y Memoria es **menor al 50%** de la capacidad asignable.
2. Todos los Pods en ese nodo pueden moverse a otros nodos.
3. El escalado hacia abajo (*scale-down*) no est√° desactivado en la configuraci√≥n.
4. **Tiempo de espera:** Si el nodo sigue siendo innecesario durante **10 minutos**, se elimina.

## 4. L√≠mites y Capacidad

| Caracter√≠stica | L√≠mite / Detalle | Notas |
| :--- | :--- | :--- |
| **Cluster Autoscaler** | Soporta hasta **15,000 nodos** | |
| **Pods por Nodo** | M√°ximo **256 pods** | |
| **L√≠mite Global de Pods** | **200,000 pods** | L√≠mite a nivel de cl√∫ster usando Autoscaler. |
| **Grandes Cl√∫steres (AI)** | Hasta **65,000 nodos** | Requiere GKE v1.31+. Usa tecnolog√≠a Spanner. <br>‚ö†Ô∏è **No soporta Cluster Autoscaler** (debe ser manual v√≠a API). |

> **‚ö†Ô∏è Importante sobre Cuotas:** Los l√≠mites est√°ndar de **Compute Engine** siguen aplicando. Si tu Autoscaler intenta crear nodos pero no tienes suficiente cuota de CPU/IPs en tu proyecto, los nuevos VMs no iniciar√°n y habr√° interrupciones.

## 5. Comandos Clave (`gcloud`)

* **Crear cl√∫ster con autoscaling:**
    `... --enable-autoscaling`
* **Crear node pool con autoscaling:**
    `... --enable-autoscaling`
* **Actualizar pool existente (Activar/Desactivar):**
    `... --enable-autoscaling` o `... --no-enable-autoscaling`

## 6. Topolog√≠a Zonal
* **Por defecto:** Todos los recursos (nodos y plano de control) est√°n en la misma zona.
* **Zonas secundarias:** Si se habilitan, todos los *node pools* se duplican en la zona secundaria (similar a los cl√∫steres regionales).
# üìç Colocaci√≥n de Pods en GKE (Pod Placement)

El "Pod Placement" es el proceso de controlar en qu√© nodos se ejecutan tus aplicaciones dentro del cl√∫ster. Esto es crucial para optimizar el rendimiento, asegurar la alta disponibilidad y gestionar la asignaci√≥n de recursos.

## 1. Comportamiento por Defecto
En **GKE Standard**, el programador (*kube-scheduler*) toma decisiones autom√°ticas bas√°ndose en:
* **Especificaciones del Pod:** Solicitudes (*Requests*) y l√≠mites (*Limits*) de recursos.
* **Capacidad del Nodo:** El scheduler dispersa los Pods autom√°ticamente en nodos que tengan espacio libre.
* **Etiquetas de Zona:** Al iniciarse, los nodos reciben etiquetas autom√°ticas (ej. zona geogr√°fica) para facilitar el rastreo, incluso en cl√∫steres multizonales.

---

## 2. nodeSelector (La forma sencilla/r√≠gida)
Es el m√©todo m√°s b√°sico para forzar a un Pod a ir a un nodo espec√≠fico. Funciona como una "lista de verificaci√≥n" simple.
Insert Image05.png
* **C√≥mo funciona:** Especificas una etiqueta (Label) en la configuraci√≥n del Pod. Si el nodo no tiene esa etiqueta exacta, el Pod **no** se programa (se queda en estado `Pending`).
* **Uso Manual:** Puedes etiquetar nodos manualmente para identificar caracter√≠sticas (ej. `disco=ssd`).
* **Autopilot:** Puedes usar `nodeSelector` para solicitar clases de c√≥mputo espec√≠ficas (ej. "Balanced") usando la etiqueta `cloud.google.com/compute-class`.
Insert Image06.png
---

## 3. Node Affinity (Afinidad de Nodo)
Es la evoluci√≥n de `nodeSelector`. Permite reglas mucho m√°s expresivas y flexibles. A diferencia del selector simple, aqu√≠ puedes definir **preferencias** (soft) adem√°s de **requisitos** (hard).

### Palabras Clave de Configuraci√≥n
Es vital entender estos t√©rminos para configurar el YAML:

| T√©rmino | Tipo | Comportamiento |
| :--- | :--- | :--- |
| **requiredDuringSchedulingIgnoredDuringExecution** | **Obligatorio (Hard)** | Si no hay un nodo que cumpla la regla, el Pod no se programa. (Equivalente estricto a `nodeSelector`). |
| **preferredDuringSchedulingIgnoredDuringExecution** | **Preferible (Soft)** | GKE intentar√° cumplir la regla. Si no puede, programar√° el Pod en cualquier otro nodo disponible. |
| **...IgnoredDuringExecution** | **Est√°tico** | Si las etiquetas de un nodo cambian *despu√©s* de que el Pod ya est√° corriendo, el Pod **no** se ver√° afectado (no se mover√°). |
Image07.png


### Sistema de Pesos (Weights)
En las reglas "Preferidas" (`Preferred`), puedes asignar un peso del **1 al 100**:
* **1:** Preferencia d√©bil.
* **100:** Preferencia muy fuerte.
* El scheduler eval√∫a los nodos, suma los puntajes y asigna el Pod al nodo con mayor puntuaci√≥n.

---

## 4. L√≥gica de Selecci√≥n y Operadores
Las reglas de afinidad utilizan l√≥gica booleana para filtrar nodos:

* **AND L√≥gico:** Si usas `NodeSelectorTerms`, el nodo debe cumplir **todas** las expresiones listadas (matchExpressions).
* **Operador `In`:** El valor de la etiqueta del nodo debe coincidir con *uno* de los valores listados en tu regla.
* **Operador `NotIn`:** Se usa para configurar reglas de **Anti-Affinity** (evitar ciertos nodos).

> **Consejo:** Se recomienda nombrar los *Node Pools* describiendo su hardware (ej. `n1-highmem-4`). GKE etiqueta autom√°ticamente los nodos con el nombre del pool, facilitando crear reglas de afinidad hacia ese hardware espec√≠fico.

Las reglas de afinidad de nodos en este ejemplo est√°n configuradas para expresar una fuerte preferencia por los nodos que est√°n en los grupos de nodos n1-highmem-4 o n1-highmem-8.Se recomienda que los nombres de los grupos de nodos indiquen el tipo de instancias de c√≥mputo que se usar√°n para crear los nodos. 
Image08.png 
---

## 5. Topolog√≠a y Afinidad Inter-Pod
Las reglas no se limitan a nodos individuales; pueden trabajar a mayor escala o basarse en la ubicaci√≥n de *otros* Pods.

### A. TopologyKeys (Dominio de Topolog√≠a)
Permite definir reglas basadas en dominios de infraestructura m√°s amplios, como zonas o regiones.
* *Ejemplo:* "No programar este Pod en la Zona A si ya hay r√©plicas corriendo all√≠" (para maximizar disponibilidad).

### B. Inter-Pod Affinity / Anti-Affinity
En lugar de mirar las etiquetas del *Nodo*, el scheduler mira las etiquetas de los *Pods* que ya est√°n ejecut√°ndose en ese nodo.

* **Affinity (Atracci√≥n):** "Quiero que mi Pod *Frontend* corra en el mismo nodo que mi Pod *Cache*" (para reducir latencia de red).
* **Anti-Affinity (Repulsi√≥n):** "No quiero dos Pods de mi *Base de Datos* en el mismo nodo f√≠sico" (para evitar que un fallo de nodo tumbe toda la DB).

# ‚õî Taints y Tolerations (Manchas y Tolerancias)

Mientras que *Affinity* y *nodeSelector* sirven para **atraer** Pods a ciertos nodos, los **Taints** sirven para **repelerlos**. Es un mecanismo de restricci√≥n.

## 1. Concepto B√°sico
* **Taint (en el Nodo):** Es una "mancha" o etiqueta especial aplicada al nodo que dice: "No acepto Pods aqu√≠, a menos que tengan un permiso especial".
* **Toleration (en el Pod):** Es la excepci√≥n o "permiso especial" configurado en el Pod que le permite ignorar el Taint y aterrizar en ese nodo.

> **Diferencia Clave:** Los Taints se configuran en los **Nodos**, mientras que Affinity/Selectors se configuran en los **Pods**.



## 2. Anatom√≠a de un Taint
Se aplica con el comando: `kubectl taint nodes ...`
Consta de tres partes:
1.  **Key (Clave):** Nombre descriptivo.
2.  **Value (Valor):** Informaci√≥n opcional.
3.  **Effect (Efecto):** Qu√© pasa si el Pod no tolera el Taint.

## 3. L√≥gica de Coincidencia (Matching)
Para que un Pod entre en un nodo con Taint, su *Toleration* debe coincidir en **Key** y **Effect**.
La validaci√≥n del **Value** depende del operador usado, pero debe pasar el **value check**:

* **Operator "Equal" (Por defecto):** El valor del Pod debe ser **id√©ntico** al valor del Taint.
* **Operator "Exists":** Solo verifica que la Key y el Effect existan (act√∫a como un comod√≠n), ignorando el contenido del valor.

Y si se aplican m√∫ltiples taints a un nodo, se impedir√° que los nuevos Pods aterricen en el nodo, y los Pods en ejecuci√≥n ser√°n desalojados.

## 4. Los 3 Efectos de Taint (Important√≠simo)

| Efecto | Comportamiento sobre Pods NUEVOS | Comportamiento sobre Pods EXISTENTES |
| :--- | :--- | :--- |
| **`NoSchedule`** | **Bloquea.** No se programan si no tienen tolerancia. | **Ignora.** Los que ya estaban corriendo siguen ah√≠ felices. |
| **`PreferNoSchedule`** | **Evita (Soft).** Intenta no ponerlos ah√≠, pero si no hay espacio en otro lado, los acepta. | **Ignora.** No afecta a los existentes. |
| **`NoExecute`** | **Bloquea.** No entran nuevos. | **Desaloja (Evict).** Expulsa inmediatamente a los Pods que no tengan la tolerancia marcada como NOExecute. |

## 5. Gesti√≥n en GKE con Node Pools
Gestionar Taints nodo por nodo es complejo. GKE simplifica esto usando **Node Pools**:

1.  Creas un pool con hardware espec√≠fico (ej. "High CPU").
2.  GKE etiqueta autom√°ticamente esos nodos con el nombre del pool.
3.  Puedes usar `nodeSelector` o `Taints` a nivel de creaci√≥n del pool para asegurar que solo las cargas de trabajo correctas (ej. Servidores Web que necesitan mucha CPU) caigan en ese hardware.

# üì¶ Herramientas de Despliegue y Gesti√≥n de Software

Aunque Google provee las herramientas, **t√∫ eres responsable** de definir los patrones de despliegue para asegurar operaciones eficientes.

## 1. El Flujo de CI/CD en Google Cloud

### A. Cloud Build (Construcci√≥n)
* **Qu√© es:** Herramienta **serverless** para CI/CD (Integraci√≥n y Despliegue Continuo).
* **Funci√≥n:** Permite construir, probar y desplegar software en varios lenguajes y entornos.
* **Ventaja:** No necesitas gestionar servidores de build (como Jenkins tradicionales).

### B. Artifact Registry (Almacenamiento)
* **Qu√© es:** Un repositorio centralizado y seguro para tus artefactos de construcci√≥n.
* **Uso principal:** Almacenar **im√°genes de contenedor**.
* **Integraci√≥n:** GKE descarga ("fetches") las im√°genes directamente desde aqu√≠ para ejecutarlas en los Pods.

---

## 2. Helm (Gestor de Paquetes)

Helm es una herramienta de c√≥digo abierto fundamental para Kubernetes.

* **Analog√≠a:** Funciona igual que `apt-get` o `yum` en Linux, pero para aplicaciones de Kubernetes.
* **Funci√≥n:** Simplifica la instalaci√≥n, actualizaci√≥n, consulta y eliminaci√≥n de recursos en el cl√∫ster hablando con el API Server.

### Concepto Clave: Helm Charts
* **Definici√≥n:** Son paquetes que agrupan m√∫ltiples objetos de Kubernetes (Deployments, Services, ConfigMaps, etc.) necesarios para una aplicaci√≥n.
* **Ventajas:**
    * Gestionan **dependencias** autom√°ticamente.
    * Permiten versionado, publicaci√≥n y uso compartido de aplicaciones complejas.
    * Reducen el error humano en despliegues manuales.
    
---

## 3. Google Cloud Marketplace
* **Qu√© ofrece:** Soluciones, "stacks" de desarrollo y servicios listos para usar.
* **Despliegue:** La instalaci√≥n est√° automatizada gracias al uso de comandos `kubectl` y **Helm Charts** por detr√°s.